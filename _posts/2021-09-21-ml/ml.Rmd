---
title: "Machine Learning"
description: |
   Machine Learning Application Method for Time Series Data
author:
  - name: Yeongeun Jeon
  - name: Jung In Seo
date: 09-21-2021
preview: preview.PNG
categories: Time Series
output: 
  distill::distill_article:
        toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

- Data 출처 : [Data Mining for Business Analytics](https://www.dataminingbook.com/book/r-edition)에서 사용한 미국 철도 회사 “Amtrak”에서 수집한 1991년 1월~2004년 3월까지 매달 환승 고객 수
- Ref. [Regression-trees for forecasting time series in R](https://petolau.github.io/Regression-trees-for-forecasting-time-series-in-R/)

-----------------      
    
    
# **Introduction**

> 트리 기반 머신 러닝 기법(Tree-based on Machine Learning Technique)을 시계열에 적용하는 방법에 대해 정리한다.

- 트리 기반 머신 러닝 기법(Tree-based on Machine Learning Technique)은 몇 가지 장점을 가지고 있어 모델링에 널리 사용되는 머신 러닝 방법이다.
    - 이해하기 쉽다.
    - 트리 구조는 시각화 하기에 간단하며, 탐색적 분석에 유용하다.
    - 빠르다.
    - 예측 정확도가 우수하다.
- 하지만, `이상치에 예민하다는 단점`이 있다. 
    - 트리는 Training Data를 바탕으로 간단한 규치만 배우면서 만들어지기 때문이다.
- 그렇기 때문에, `트리 기반 머신러닝 기법은 추세를 포착할 수 없다.`
    - Ref. [1](https://petolau.github.io/Regression-trees-for-forecasting-time-series-in-R/), [2](https://dohk.tistory.com/220), [3](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3), [4](https://stats.stackexchange.com/questions/246853/time-series-and-xgboost), [5](https://stackoverflow.com/questions/45695975/xgboost-time-series-model-does-not-capture-trend)
- 그래서 `분해(Decomposition)`을 이용하여, `추세(Trend)가 제거된 Detrend Data에 트리 기반 머신 러닝 기법을 적용`해야한다.
    - 추세는 적절한 시계열 모형을 이용하여 예측한다.
- `최종 예측 = 머신 러닝의 예측값 + 추세의 예측값 ` 


---------

# **Application**

- Ridership on Amtrak Trains(미국 철도 회사 “Amtrak”에서 수집한 1991년 1월~2004년 3월까지 매달 환승 고객 수) 예제를 이용하여 트리 기반 머신 러닝 기법이 실제 데이터에 어떻게 적용되는지 설명한다.

----------

## **Data 불러오기**

```{r}
pacman::p_load( "dplyr", "xts", 
                "caret", "caretEnsemble",
                "forecast", 
                "ggplot2")

library(doParallel)
library(parallel)

# cl <- makePSOCKcluster(detectCores())
# clusterEvalQ(cl, library(foreach))
# registerDoParallel(cores=cl)

registerDoParallel(cores=detectCores())

# In Mac
# guess_encoding("Amtrak.csv")
# Amtrak.data <- read.csv("Amtrak.csv", fileEncoding="EUC-KR")

Amtrak.data <- read.csv("C:/Users/User/Desktop/Amtrak.csv")
ridership.ts <- ts(Amtrak.data$Ridership, start=c(1991,1), end=c(2004,3), freq=12)

```

------------

## **Data 분할**

```{r}
train.ts <- window(ridership.ts,start=c(1991,1), end=c(2001,3))
test.ts  <- window(ridership.ts,start=c(2001,4))
n.test    <- length(test.ts)
```


------------

## **예측 변수 생성**

- 머신 러닝 기법을 적용하기 위해 예측 변수(Predictor Variable)를 생성하였다.
    - 퓨리에 항(Fourier Terms)
    - 달(Month)

```{r}
# 1. Fourier Term
FT      <- fourier(train.ts, K=2)                    # K : sine, cosine 쌍의 개수/시계열 데이터의 계절 주기가 2개 이상일 때, K는 계절 주기 수만큼 필요
FT.Test <- fourier(train.ts, K=2, h=n.test)

# 2. Month
xts(ridership.ts, order = as.Date(ridership.ts))
Month  <- as.Date(ridership.ts) %>%                  # Date 추출
  lubridate::month()                                 # Month 추출

## 퓨리에 항과 합치기
Train.X <- cbind("Month"= Month[1:length(train.ts)], FT) 
Test.X  <- cbind("Month"= Month[-(1:length(train.ts))], FT.Test)  
```


------------

## **분해**

- 트리 기반 머신러닝 기법은 추세를 포착하기 못하기 떄문에, 추세를 제거한 Detrend Data를 필요로 한다.
- Seasonal and Trend decomposition using Loess (STL) 분해를 이용하여 시계열 데이터를 추세, 계절성과 나머지 성분으로 분해하였다.
    - 만약, 시계열 데이터가 다중 계절성을 갖는다면, `mstl()`을 이용할 수 있다.
- 추세를 제거하고 `계절성과 나머지 성분을 더한 값을 Target으로 한 Training Dataset`을 생성한다.    


```{r}
decomp.ts <- stl(train.ts, , s.window = "periodic", robust = TRUE)$time.series 
# decomp.ts <- mstl(Power.msts, s.window = "periodic", robust = TRUE) # 다중 계절성인 경우

# Target without Trend
Target <- decomp.ts %>% 
  data.frame %>%
  rowwise() %>%                                        # 행별로 작업
  dplyr::mutate(y=sum( seasonal, remainder )) %>%      # Target = Season + Remainder => Detrend
  dplyr::select(y)


Train.Data <- cbind(Target, Train.X)
```

------------

## **머신 러닝 적용**

- 위에서 생성한 Training Dataset에 머신 러닝 기법을 적용한다.
- `caret` package를 이용하여, 가장 대표적인 `랜덤포레스트(Random Forest)`와 `eXtreme Gradient Boosting(XGBoost)`를 적용해보았다.
    - Hyperparameter는 Random Search를 이용하여 최적의 조합을 찾았다.
- 게다가, `caretEnsemble` package를 이용하여, 앙상블(Ensemble) 기법 `Stacking`도 적용해보았다.
    
-------------

### **Random Forest**

- Bagging을 이용한 트리 기반 모형이다.
- 나무를 분할할 때 랜덤적으로 후보 예측 변수를 선택함으로써, 생성된 나무들의 연관성은 감소된다.
- Random Forest에서 Hyperparameter는 `mtry`로 트리가 분할될 때 랜덤적으로 선택되는 후보군 예측 변수 갯수이다.


```{r}
set.seed(100)
fitControl <- trainControl(method = "adaptive_cv",   # cv, repeatedcv
                           number = 5,
                           repeats = 5,
                           adaptive = list(min = 5,
                                           alpha = 0.05,
                                           method = "BT",
                                           complete = TRUE),
                           search = "random",
                           allowParallel = TRUE) 

RF <- function(train, tuneLength, ntree = 500, nodesize = 5){
  
  set.seed(100)                                        # seed 고정 For Cross Validation
  caret.rf <- caret::train(y~., data = train, 
                           method = "parRF",           # Tune Parameter : mtry
                           trControl = fitControl,
                           tuneLength = tuneLength,   
                           ntree = ntree,             
                           nodesize = nodesize,        # nodesize : Terminal Node의 최소 크기
                           importance = TRUE)   
  
  return(caret.rf)
  
}

RF.Caret <- RF(Train.Data, 
               tuneLength = 2,     # tuneLength (탐색할 후보 모수 갯수)
               ntree = 100)        # ntree : 생성할 Tree 수

RF.Caret
RF.Caret$finalModel
RF.Caret$finalModel$tuneValue
```

-------------

### **XGBoost**

- Boosting을 이용한 트리 기반 모형이다.
- 손실함수와 경사하강법을 이용하는 Gradient Boosting의 단점을 해결하기 위해 제안되었다.
- 가장 큰 특징으로는, 병렬 처리로 인해 빠르고 조기 종료가 가능하다는 것이다.
- XGBoost에서 Hyperparameter는 다음과 같다.
   - `nrounds` : 반복 수
   - `max_depth` : Tree의 최대 깊이
   - `eta` : Learning Late
   - `gamma` : 분할하기 위해 필요한 최소 손실 감소, 클수록 분할이 쉽게 일어나지 않음
   - `colsample_bytree` : Tree 생성 때 사용할 예측변수 비율 
   - `min_child_weight` : 한 leaf 노드에 요구되는 관측치에 대한 가중치의 최소 합
   - `subsample` : 모델 구축시 사용할 Data비율로 1이면 전체 Data 사용

```{r}
set.seed(100)
fitControl <- trainControl(method = "adaptive_cv",   # cv, repeatedcv
                           number = 5,
                           repeats = 5,
                           adaptive = list(min = 5,
                                           alpha = 0.05,
                                           method = "BT",
                                           complete = TRUE),
                           search = "random",
                           allowParallel = TRUE) 

XGBoost <- function(train, tuneLength){
  
  set.seed(100)                                         # seed 고정 For Cross Validation
  caret.xgb <- caret::train(y~., data = train, 
                            method = "xgbTree",          
                            trControl = fitControl,
                            # objective = "reg:squarederror", # error(The following parameters were provided multiple times)
                            tuneLength = tuneLength     # tuneLength (탐색할 후보 모수 갯수)
  )   
  
  return(caret.xgb)
  
}

XGB.Caret <- XGBoost(Train.Data, 2)
XGB.Caret
XGB.Caret$finalModel
XGB.Caret$finalModel$tuneValue
```

-------------

### **Stacking**

- Stacking은 앙상블 기법 중 하나로, 비슷한 성능을 가진 모형들을 결합함으로써, 예측 성능을 항상 시키기 위해 적용될 수 있다.
- Stacking은 두 종류의 모형들이 필요하다.
    - Individual Model : 원본 Training Data을 이용하여 학습할 모형
        - Individual Model은 `여러개의 서로 다른 머신 러닝 알고리즘로 구성`
    - Final Model : Individual Model들의 예측 결과를 결합한 Data를 Training Data로 사용하여 학습할 모형
- Stacking의 적용 알고리즘은 다음과 같다.
    - 원본 Training Data에 `서로 다른 머신 러닝 알고리즘`을 이용하여 예측 결과를 생성한다.
    - 생성된 예측 결과를 결합하여 Final Model에 대한 Training Data를 생성한다.
    - 위에서 생성된 Training Data를 이용하여 Final Model을 학습시킨다.
    - 원본 Training Data를 기반으로 학습한 모형에 원본 Test Data를 이용하여 예측 결과를 생성한다.
    - 생성된 예측 결과를 결합하여 Final Model에 대한 Test Data를 생성한다.
    - 위에서 생성된 Test Data를 이용하여 Final Model의 예측 결과와 원본 Test Data의 Target과 비교한다.
- 자세한 코드는 [여기](https://github.com/zachmayer/caretEnsemble/blob/master/R/caretStack.R)를 참조한다.    
- 앞써, 원본 Training Data에 적합시킨 Random Forest와 XGBoost의 Hyperparameter 결과를 기반으로 생성된 예측 결과들을 결합하여 스태킹하였다.   
- `caretEnsemble` package의 `caretList()`를 이용하여, Individual Model을 선언하고, `caretStack()`으로 Final Model을 선언한다.
    
    
```{r}
# Ref. https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html
#      https://github.com/zachmayer/caretEnsemble/blob/master/R/caretStack.R


# 1. Modeling for Stacking (Declare Individual Model)
set.seed(100)
fitControl <- trainControl(method = "repeatedcv",        # adaptive_cv
                           number = 5,
                           repeats = 5,
                           # adaptive = list(min = 5,
                           #                 alpha = 0.05,
                           #                 method = "BT",
                           #                 complete = TRUE),
                           # search = "random",            # grid
                           savePredictions = "final",      # 최적 모수에 대한 예측 저장
                           # classProbs = TRUE,            # 각 클래스에 대한 확률 저장(Classification)
                           index = createResample(Train.Data$y, 1),  # index : 각 resapling에 대한 요소, Training에 사용되는 행 번호/ createResample : 붓스트랩
                           allowParallel = TRUE) 


# 원본 Training Data에 학습시킨 Hyperparameter 결과
alg_tune_list <- list(                            # Do not use custom names in list. Will give prediction error with greedy ensemble. Bug in caret.
  parRF = caretModelSpec(method="parRF",
                         importance = TRUE,
                         nodeside = 5,
                         tuneGrid = expand.grid(mtry=RF.Caret$finalModel$tuneValue$mtry)),
  xgbTree = caretModelSpec(method="xgbTree",
                           tuneGrid = expand.grid(nrounds = XGB.Caret$finalModel$tuneValue$nrounds,
                                                  max_depth = XGB.Caret$finalModel$tuneValue$max_depth,
                                                  eta = XGB.Caret$finalModel$tuneValue$eta,
                                                  gamma = XGB.Caret$finalModel$tuneValue$gamma,
                                                  colsample_bytree = XGB.Caret$finalModel$tuneValue$colsample_bytree,
                                                  min_child_weight = XGB.Caret$finalModel$tuneValue$min_child_weight,
                                                  subsample = XGB.Caret$finalModel$tuneValue$subsample)))

set.seed(100)
multi_mod <- caretList(y~., data = Train.Data, trControl = fitControl, 
                       tuneList = alg_tune_list)  # search = "grid"     


multi_mod$parRF
multi_mod$xgbTree
```
    

```{r}
# 2. Stacking (개별 모형들의 예측값을 결합한 Data를 Training data로 쓰는 Final Model)

set.seed(100)
stackControl <- trainControl(method = "adaptive_cv",
                             number = 5,
                             repeats = 5,
                             adaptive = list(min = 5,
                                             alpha = 0.05,
                                             method = "BT",
                                             complete = TRUE),
                             search = "random",
                             allowParallel = TRUE) 

set.seed(100)
stack.xgb <- caretStack(multi_mod, method = "xgbTree",  # Final Model
                        trControl = stackControl, 
                        tuneLength = 2)                 # 모수 후보 갯수


stack.xgb
stack.xgb$ens_model$finalModel
stack.xgb$ens_model$finalModel$tuneValue
```
    
-------------

#### **Stacking with GLM**

- Stacking의 Final Model를 Generalized Linear Model(GLM)으로 하면, Individual Model들의 예측 결과들에 가중치를 곱하여 더한 것과 같다. 
- `caretEnsemble` package의 `caretEnsemble()`를 이용하면 Final Model이 `GLM`이며, [여기](https://github.com/zachmayer/caretEnsemble/blob/master/R/caretEnsemble.R)의 R code를 확인하면 된다.
- 최종 예측 결과 $\hat{Y}$는 다음과 같다.

$$
\begin{aligned}
	       \hat{Y} = \beta_{0} + \sum^{m}_{i=1}  \hat{y}_{i}
\end{aligned}
$$

- $m$ : Individual Model 개수
- $\hat{y}_{i}$ : $i$번째 Individual Model의 예측 결과


```{r}
set.seed(100)
greedyEnsemble <- caretEnsemble(multi_mod, trControl = trainControl(method = "cv", number=5))
greedyEnsemble
greedyEnsemble$ens_model$finalModel
summary(greedyEnsemble)
```

--------


##### **변수 중요도**

```{r}
VI <- varImp(greedyEnsemble) 
varImp(greedyEnsemble)

VI$x <- as.character(row.names(VI))
VI2 <- reshape2::melt(VI, id.vars="x",
                      variable.name="Type",
                      value.name='VI')

ggplot(VI2, aes(x=reorder(x, VI), y=VI)) + 
  geom_bar(stat="identity", position=position_dodge(), show.legend = F) + 
  facet_wrap(.~ Type, nrow=1) +
  xlab("") +      
  ylab("") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.x = element_text(size = 13),  # angle = 45, vjust = 1, hjust = 1
        axis.text.y = element_text(size = 13),
        axis.title = element_blank(),
        plot.title = element_blank(),
        legend.title = element_blank(),
        strip.text.x = element_text(size=15, color="black"))

```


---------

## **예측**

- 최종 예측은 추세의 예측값과 머신 러닝 기법에 의한 예측값을 더하여 생성된다.
    - 추세는 시계열 모형 (예: ARIMA)을 이용하여 예측

---------

### **추세 예측**

- 여기에서는 ARIMA 모형을 이용하여 추세를 예측하였다.

```{r}
# Trend 
trend.part        <- data.frame(decomp.ts)$trend %>% # Only Trend of Training Data
  ts()

# Fitting ARIMA for Trend 
trend.fit.arima   <- auto.arima(trend.part)

# Forecast 
trend.arima.pred  <- forecast(trend.fit.arima, n.test)
trend.arima.pred$mean 

```


----------

### **최종 예측**


```{r}
# 최종 예측(Seasonal + Remainder + Trend)
Pred.RF        <- predict(RF.Caret, Test.X) + trend.arima.pred$mean   
Pred.XGB       <- predict(XGB.Caret, Test.X) + trend.arima.pred$mean
stack.Pred.XGB <- predict(stack.xgb, Test.X) + trend.arima.pred$mean
stack.Pred.GLM <- predict(greedyEnsemble, Test.X) + trend.arima.pred$mean
```



```{r}
# Accuracy
acc_RF        <- accuracy(c(Pred.RF), test.ts)
acc_XGB       <- accuracy(c(Pred.XGB), test.ts)
acc_stack.XGB <- accuracy(c(stack.Pred.XGB), test.ts)
acc_stack.GLM <- accuracy(c(stack.Pred.GLM), test.ts)

acc_RF
acc_XGB
acc_stack.XGB
acc_stack.GLM
```

